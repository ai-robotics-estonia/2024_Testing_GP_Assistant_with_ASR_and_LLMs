## Dependencies

This project requires Python **3.12** or higher.

All necessary Python packages are listed in the `requirements.txt` file.


To install the required packages, run:

```bash
pip install -r requirements.txt
```


## .env File

This project requires a `.env` file to store your API key securely.

Create a `.env` file in the directory you plan to run the notebooks in with your API key like this:

~~~
API_KEY=your_api_key_here
~~~

The `.env` file is used to load the API key into the application without hardcoding it in the source code.


## Creating transcripts

This project uses the [est-asr-pipeline](https://github.com/taltechnlp/est-asr-pipeline) from the TalTech NLP group to perform Estonian automatic speech recognition (ASR).

To generate transcripts from audio using the full Nextflow pipeline, use the following command:

```bash
NXF_VER=22.10.0 nextflow run transcribe.nf --in /path/to/some_audiofile.mp3
```

Alternatively, you can use the `create_transcripts.ipynb` notebook, which uses the `whisper-medium-et` model to generate transcripts without relying on the est-asr-pipeline.


## File Structure and Naming Conventions

To run the code in the notebooks exactly as provided, certain files must follow specific naming conventions and be placed in the correct directories.

### Required Directories

Ensure the following directories exist:

- `prompts/`  
  Contains the different kinds of prompts used throughout the notebooks. 

- `est_asr_transcripts/`  
  Stores the transcripts generated by the [est-asr-pipeline](https://github.com/taltechnlp/est-asr-pipeline).

- `summaries/`  
  This directory will hold the generated summaries.  
  All required subdirectories inside `summaries/` will be created automatically by the code when generating summaries.

### Data Structure for Physician Files

The audio and summary data provided by physicians must follow this structure:

- `Arst_n/` — A folder for each doctor (where `n` is a three-digit number, e.g. `Arst_001`, `Arst_002`, etc.)

  - `Patsient_m/` — A folder for each patient under a specific doctor (where `m` is a three-digit number, e.g. `Patsient_001`, `Patsient_002`, etc.)

    Each patient folder must include the following two files:

    - `arsti_salvestus_orig_n_m.wav`  
      The audio file of the consultation.  
      Here, `n` and `m` are two-digit numbers (e.g. `arsti_salvestus_orig_01_02.wav` for doctor 01 and patient 02).

    - `arsti_kokkuvote_orig_n_m.txt`  
      The text file containing the doctor’s written summary of the visit.  
      Follows the same two-digit numbering convention (e.g. `arsti_kokkuvote_orig_01_02.txt`).
    - A subfolder named `parandatud/`, which may contain a corrected `.txt` file for calculating the Word Error Rate (WER) against the generated transcripts.

Following this structure is essential for the notebooks to run correctly without modification.

## Notebook Usage Order

To use this project effectively, follow the recommended order of the notebooks:

1. **(Optional)** `create_voice_change.ipynb`  
   Use this notebook if you want to test voice masking for anonymizing audio files.

2. **`create_transcripts.ipynb`**  
   Generate transcripts using the `whisper-medium-et` model, if they haven't already been created using the est-asr-pipeline.

3. **(Optional)** `compute_WER.ipynb`  
   If you have access to a corrected transcript (in the `parandatud/` folder), use this notebook to calculate the Word Error Rate (WER).

4. **`create_summaries.ipynb`**  
   Generate summaries of the transcripts using your configured prompt and summarization pipeline. 
   _Note: One of the prompts requires you to have 10 different summaries from the same doctor (i.e., summaries from 10 different patients authored by the same physician)._

5. **`compute_bertscore.ipynb`**  
   Compare generated summaries with the doctor's original summaries using BERTScore.

6. **`LLM-as-judge.ipynb`**  
   Use an LLM to judge the quality of a summary compared to the doctor's original summary.

7. **`LLM-as-judge-batch.ipynb`**  
   Use this notebook for batch evaluation when comparing a large number of summaries.

