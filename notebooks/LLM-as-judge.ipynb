{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "load_dotenv()  # Loads variables from .env into environment\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "#os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(\n",
    "    api_key=api_key,  # This is the default and can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class TruePositive(BaseModel):\n",
    "    A_fact: str\n",
    "    B_fact: str\n",
    "\n",
    "\n",
    "class FalsePositive(BaseModel):\n",
    "    A_fact: str\n",
    "\n",
    "\n",
    "class FalseNegative(BaseModel):\n",
    "    B_fact: str\n",
    "\n",
    "\n",
    "class SummaryCounts(BaseModel):\n",
    "    TP: int\n",
    "    FP: int\n",
    "    FN: int\n",
    "\n",
    "\n",
    "class ValidationResponse(BaseModel):\n",
    "    true_positives: list[TruePositive]\n",
    "    false_positives: list[FalsePositive]\n",
    "    false_negatives: list[FalseNegative]\n",
    "    summary_counts: SummaryCounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLLMValidation(instruction_prompt, summary_A, summary_B, print_prompt=False):\n",
    "    full_prompt = f\"{instruction_prompt}\\nSummary A:\\n{summary_A}\\nSummary B:\\n{summary_B}\"\n",
    "    if print_prompt:\n",
    "        print(full_prompt)\n",
    "\n",
    "    # Step 2: Generate summary based on the transcript using the guidelines\n",
    "    validation_response = client.beta.chat.completions.parse(\n",
    "        model=\"o3-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        #temperature=0.0000001, #o3-mini doesn't allow this value\n",
    "        #top_p=0.0000001, #also not allowed\n",
    "        seed=1234,\n",
    "        reasoning_effort=\"high\",\n",
    "        response_format=ValidationResponse,  # Ensures JSON output\n",
    "        #tool_choice=\"auto\",  # Uses JSON schema\n",
    "    )\n",
    "    return validation_response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def ReadFile(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            txt = file.read().strip()\n",
    "            return txt\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error: {file_path} not found!\")\n",
    "    \n",
    "\n",
    "\n",
    "def SaveValidation(result, folder_path, doctor, patient, prompt_number, model):\n",
    "    doctor_str = f\"{doctor:02}\"\n",
    "    patients_str = f\"{patient:02}\"\n",
    "    output_file_name = f\"arst_{doctor_str}_patsient_{patients_str}_v천rdlus_prompt_{prompt_number}_{model}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "\n",
    "    # Convert result to string if it's not already a string\n",
    "    if isinstance(result, str):\n",
    "        result_str = result\n",
    "    else:\n",
    "        # Assuming the result can be serialized to JSON\n",
    "        try:\n",
    "            result_str = json.dumps(result, ensure_ascii=False, indent=4)  # Serialize to JSON string\n",
    "        except TypeError:\n",
    "            # If it cannot be serialized to JSON, fall back to its string representation\n",
    "            result_str = str(result)  # Convert the object to its string representation\n",
    "\n",
    "    # Check if result is JSON format\n",
    "    try:\n",
    "        # Try parsing the string to check if it's valid JSON\n",
    "        result_json = json.loads(result_str)\n",
    "        \n",
    "        # Save the parsed result as a JSON file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(result_json, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Validation saved to {output_file_path}\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        # If result is not in JSON format, save it as a plain text file\n",
    "        output_file_name = f\"arst_{doctor_str}_patsient_{patients_str}_v천rdlus_prompt_{prompt_number}_{model}.txt\"\n",
    "        output_file_path = os.path.join(folder_path, output_file_name)\n",
    "        \n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(result_str)  # Write the string representation of the result\n",
    "        \n",
    "        print(f\"Validation saved to {output_file_path}\")\n",
    "\n",
    "def CreateDirectory(path, folder_name):\n",
    "    \"\"\"\n",
    "    Creates a directory with the given folder_name in the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The base path where the directory should be created (can be relative).\n",
    "    folder_name (str): The name of the directory to create.\n",
    "    \"\"\"\n",
    "    # Construct the full directory path\n",
    "    full_path = os.path.join(path, folder_name)\n",
    "    # Create the directory\n",
    "    try:\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        print(f\"Directory '{full_path}' created successfully.\")\n",
    "        return full_path\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{full_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1-score\n",
    "def calculate_metrics(tp: int, fp: int, fn: int) -> Dict[str, float]:\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1_score}\n",
    "\n",
    "def average_metrics(metrics1: Dict[str, float], metrics2: Dict[str, float]) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"Precision\": (metrics1[\"Precision\"] + metrics2[\"Precision\"]) / 2,\n",
    "        \"Recall\": (metrics1[\"Recall\"] + metrics2[\"Recall\"]) / 2,\n",
    "        \"F1-score\": (metrics1[\"F1-score\"] + metrics2[\"F1-score\"]) / 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare a specific summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_prompt = ReadFile(\"LLM_as_judge_prompt_5.txt\")\n",
    "model = \"o3-mini-high\"\n",
    "doctor_index = 1\n",
    "patient_index = 1\n",
    "prompt_index = 1\n",
    "\n",
    "generated = ReadFile(f\"summaries/arst_{doctor_index}_patsient_{patient_index}/kokkuv천tted/prompt_{prompt_index}/arst_{doctor_index:02}_patsient_{patient_index:02}_kokkuv천te_prompt_{prompt}_a.txt\")\n",
    "original = ReadFile(f\"Arst_{doctor_index:03}/Patsient_{patient_index:03}/toorfailid/arsti_kokkuvote_orig_{doctor_index:02}_{patient_index:02}.txt\")\n",
    "print(f\"Processing doctor: {doctor_index}, patient: {patient_index}.\")\n",
    "response = GetLLMValidation(input_prompt, generated, original, print_prompt=False)\n",
    "print(response)\n",
    "\n",
    "\n",
    "print(response.summary_counts.TP)\n",
    "print(response.summary_counts.FP)\n",
    "print(response.summary_counts.FN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run validation one way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process LLM response and append results to CSV\n",
    "def process_and_save_results(doctor_index: int, patient_index: int, prompt_index: int, response, csv_filename=\"o3-mini-high-validations.csv\"):\n",
    "    # Extract TP, FP, FN from response\n",
    "    tp = response.summary_counts.TP\n",
    "    fp = response.summary_counts.FP\n",
    "    fn = response.summary_counts.FN\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    metrics = calculate_metrics(tp, fp, fn)\n",
    "\n",
    "    # Format results for CSV\n",
    "    row = [doctor_index, patient_index, \n",
    "           round(metrics[\"Precision\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(metrics[\"Recall\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(metrics[\"F1-score\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(metrics[\"Precision\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(metrics[\"Recall\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(metrics[\"F1-score\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(metrics[\"Precision\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(metrics[\"Recall\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(metrics[\"F1-score\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(metrics[\"Precision\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(metrics[\"Recall\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(metrics[\"F1-score\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(metrics[\"Precision\"], 3) if prompt_index == 5 else \"\", \n",
    "           round(metrics[\"Recall\"], 3) if prompt_index == 5 else \"\", \n",
    "           round(metrics[\"F1-score\"], 3) if prompt_index == 5 else \"\"]\n",
    "    \n",
    "    # Append results to CSV file\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")  # Use tab separator\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Function to run evaluation for all doctors, patients, and prompts\n",
    "def run_evaluation(csv_filename=\"o3-mini-high-validations.csv\",):\n",
    "    input_prompt = ReadFile(\"LLM_as_judge_prompt_4.txt\")\n",
    "    # Write the header only once at the start\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Doctor\", \"Patient\", \"Precision (Prompt 1)\", \"Recall (Prompt 1)\", \"F1-score (Prompt 1)\", \n",
    "                         \"Precision (Prompt 2)\", \"Recall (Prompt 2)\", \"F1-score (Prompt 2)\", \n",
    "                         \"Precision (Prompt 3)\", \"Recall (Prompt 3)\", \"F1-score (Prompt 3)\",\n",
    "                         \"Precision (Prompt 4)\", \"Recall (Prompt 4)\", \"F1-score (Prompt 4)\",\n",
    "                         \"Precision (Prompt 5)\", \"Recall (Prompt 5)\", \"F1-score (Prompt 5)\"])\n",
    "\n",
    "    # Loop over all prompts, doctors, and patients\n",
    "    for prompt_index in range(1, 6):  # Prompts 1 to 5\n",
    "        print(f\"Processing Prompt {prompt_index}...\")\n",
    "\n",
    "        for doctor_index in range(1, 11):  # Doctors 1 to 10\n",
    "            for patient_index in range(1, 11):  # Patients 1 to 10\n",
    "                start = time.time()\n",
    "                print(f\"Evaluating Doctor {doctor_index}, Patient {patient_index}, Prompt {prompt_index}\")\n",
    "\n",
    "                ai_summary_path = f\"summaries/arst_{doctor_index}_patsient_{patient_index}/kokkuv천tted/prompt_{prompt_index}/arst_{doctor_index:02}_patsient_{patient_index:02}_kokkuv천te_prompt_{prompt_index}_a.txt\"\n",
    "\n",
    "                # Load AI and doctor summaries (replace with actual file reading logic)\n",
    "                doctor_summary_path = f\"Arst_{doctor_index:03}/Patsient_{patient_index:03}/toorfailid/arsti_kokkuvote_orig_{doctor_index:02}_{patient_index:02}.txt\"\n",
    "                \n",
    "                ai_summary = ReadFile(ai_summary_path)\n",
    "                doctor_summary = ReadFile(doctor_summary_path)\n",
    "\n",
    "                # Get validation response from LLM\n",
    "                response = GetLLMValidation(input_prompt, ai_summary, doctor_summary, print_prompt=False)\n",
    "\n",
    "                if response:\n",
    "                    new_path = f\"arst_{doctor_index:02}_patsient_{patient_index:02}\"\n",
    "                    save_path = CreateDirectory(\"validations\",new_path)\n",
    "                    SaveValidation(response,save_path, doctor_index, patient_index, prompt_index, \"o3-mini-high\")\n",
    "                    process_and_save_results(doctor_index, patient_index, prompt_index, response, csv_filename)\n",
    "                    end = time.time()\n",
    "                    print(f\"Time elapsed: {end-start}\")\n",
    "\n",
    "# Run the evaluation for all doctors, patients, and prompts\n",
    "run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running validation 2 times, both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process LLM response and append results to CSV\n",
    "def process_and_save_results(doctor_index: int, patient_index: int, prompt_index: int, response1, response2, csv_filename=\"o3-mini-high-validations.csv\"):\n",
    "    # Extract TP, FP, FN from response\n",
    "    tp1 = response1.summary_counts.TP\n",
    "    fp1 = response1.summary_counts.FP\n",
    "    fn1 = response1.summary_counts.FN\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    metrics1 = calculate_metrics(tp1, fp1, fn1)\n",
    "\n",
    "    tp2 = response2.summary_counts.TP\n",
    "    fp2 = response2.summary_counts.FP\n",
    "    fn2 = response2.summary_counts.FN\n",
    "    #be cause we flipped the values, we need to change what metrics we use to calculate\n",
    "    metrics2 = calculate_metrics(tp2, fn2, fp2)\n",
    "\n",
    "    avg_metrics = average_metrics(metrics1, metrics2)\n",
    "\n",
    "    # Format results for CSV\n",
    "    row = [doctor_index, patient_index, \n",
    "           round(avg_metrics[\"Precision\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(avg_metrics[\"Recall\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(avg_metrics[\"F1-score\"], 3) if prompt_index == 1 else \"\", \n",
    "           round(avg_metrics[\"Precision\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(avg_metrics[\"Recall\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(avg_metrics[\"F1-score\"], 3) if prompt_index == 2 else \"\", \n",
    "           round(avg_metrics[\"Precision\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(avg_metrics[\"Recall\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(avg_metrics[\"F1-score\"], 3) if prompt_index == 3 else \"\", \n",
    "           round(avg_metrics[\"Precision\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(avg_metrics[\"Recall\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(avg_metrics[\"F1-score\"], 3) if prompt_index == 4 else \"\", \n",
    "           round(avg_metrics[\"Precision\"], 3) if prompt_index == 5 else \"\", \n",
    "           round(avg_metrics[\"Recall\"], 3) if prompt_index == 5 else \"\", \n",
    "           round(avg_metrics[\"F1-score\"], 3) if prompt_index == 5 else \"\"]\n",
    "    \n",
    "    # Append results to CSV file\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")  # Use tab separator\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Function to run evaluation for all doctors, patients, and prompts\n",
    "def run_evaluation(csv_filename=\"o3-mini-high-validations10.csv\",):\n",
    "    input_prompt = ReadFile(\"LLM_as_judge_prompt_5.txt\")\n",
    "    # Write the header only once at the start\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Doctor\", \"Patient\", \"Precision (Prompt 1)\", \"Recall (Prompt 1)\", \"F1-score (Prompt 1)\", \n",
    "                         \"Precision (Prompt 2)\", \"Recall (Prompt 2)\", \"F1-score (Prompt 2)\", \n",
    "                         \"Precision (Prompt 3)\", \"Recall (Prompt 3)\", \"F1-score (Prompt 3)\",\n",
    "                         \"Precision (Prompt 4)\", \"Recall (Prompt 4)\", \"F1-score (Prompt 4)\",\n",
    "                         \"Precision (Prompt 5)\", \"Recall (Prompt 5)\", \"F1-score (Prompt 5)\"])\n",
    "\n",
    "    # Loop over all prompts, doctors, and patients\n",
    "    for prompt_index in range(1, 6):  # Prompts 1 to 5\n",
    "        print(f\"Processing Prompt {prompt_index}...\")\n",
    "\n",
    "        for doctor_index in range(1, 11):  # Doctors 1 to 10\n",
    "            for patient_index in range(1, 11):  # Patients 1 to 10\n",
    "                start = time.time()\n",
    "                print(f\"Evaluating Doctor {doctor_index}, Patient {patient_index}, Prompt {prompt_index}\")\n",
    "\n",
    "                ai_summary_path = f\"summaries/arst_{doctor_index}_patsient_{patient_index}/kokkuv천tted/prompt_{prompt_index}/arst_{doctor_index:02}_patsient_{patient_index:02}_kokkuv천te_prompt_{prompt_index}_a.txt\"\n",
    "\n",
    "                # Load AI and doctor summaries (replace with actual file reading logic)\n",
    "                doctor_summary_path = f\"Arst_{doctor_index:03}/Patsient_{patient_index:03}/toorfailid/arsti_kokkuvote_orig_{doctor_index:02}_{patient_index:02}.txt\"\n",
    "                \n",
    "                ai_summary = ReadFile(ai_summary_path)\n",
    "                doctor_summary = ReadFile(doctor_summary_path)\n",
    "\n",
    "                # Get validation response from LLM\n",
    "                print(\"Running first validation\")\n",
    "                response1 = GetLLMValidation(input_prompt, ai_summary, doctor_summary, print_prompt=False)\n",
    "                print(\"Running with summaries swapped\")\n",
    "                response2 = GetLLMValidation(input_prompt, doctor_summary, ai_summary, print_prompt=False)\n",
    "\n",
    "                if response1 and response2:\n",
    "                    new_path = f\"arst_{doctor_index:02}_patsient_{patient_index:02}\"\n",
    "                    save_path = CreateDirectory(\"validations\",new_path)\n",
    "                    SaveValidation(response1,save_path, doctor_index, patient_index, prompt_index, \"o3-mini-high-AI10-first\")\n",
    "                    SaveValidation(response2,save_path, doctor_index, patient_index, prompt_index, \"o3-mini-high-doctor10-first\")\n",
    "                    process_and_save_results(doctor_index, patient_index, prompt_index, response1, response2, csv_filename)\n",
    "                    end = time.time()\n",
    "                    print(f\"Time elapsed: {end-start}\")\n",
    "\n",
    "# Run the evaluation for all doctors, patients, and prompts\n",
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aire-medisoft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
