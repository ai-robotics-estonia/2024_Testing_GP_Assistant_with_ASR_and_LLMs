{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()  # Loads variables from .env into environment\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,  # This is the default and can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON structure for the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_response = {\n",
    "                    \"true_positives\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"A_fact\": { \"type\": \"string\" },\n",
    "                                \"B_fact\": { \"type\": \"string\" }\n",
    "                            },\n",
    "                            \"required\": [\"A_fact\", \"B_fact\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"false_positives\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"A_fact\": { \"type\": \"string\" }\n",
    "                            },\n",
    "                            \"required\": [\"A_fact\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"false_negatives\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"B_fact\": { \"type\": \"string\" }\n",
    "                            },\n",
    "                            \"required\": [\"B_fact\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"summary_counts\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"TP\": {\"type\": \"integer\"},\n",
    "                            \"FP\": {\"type\": \"integer\"},\n",
    "                            \"FN\": {\"type\": \"integer\"}\n",
    "                        },\n",
    "                        \"required\": [\"TP\", \"FP\", \"FN\"]\n",
    "                    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFile(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            txt = file.read().strip()\n",
    "            return txt\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error: {file_path} not found!\")\n",
    "    \n",
    "\n",
    "def make_batch_input_jsonl(prompt_text: str, output_path: str, model: str = \"o3-mini\"):\n",
    "    \"\"\"\n",
    "    prompt_text: instruction prompt (str)\n",
    "    output_path: where to write batch .jsonl file\n",
    "    model: OpenAI model name\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as batch_file:\n",
    "        for doctor_index in range(1, 11):  # 10 doctors\n",
    "            for patient_index in range(1, 11):  # 10 patients\n",
    "                for prompt_index in range(1, 6):  # 5 prompts\n",
    "                    try:\n",
    "                        doc_path = f\"Arst_{doctor_index:03}/Patsient_{patient_index:03}/toorfailid/arsti_kokkuvote_orig_{doctor_index:02}_{patient_index:02}.txt\"\n",
    "\n",
    "                        ai_path = f\"summaries/arst_{doctor_index}_patsient_{patient_index}/kokkuvõtted/prompt_{prompt_index}/arst_{doctor_index:02}_patsient_{patient_index:02}_kokkuvõte_prompt_{prompt_index}_a.txt\"\n",
    "                                \n",
    "                        summary_A = ReadFile(ai_path)\n",
    "                        summary_B = ReadFile(doc_path)\n",
    "\n",
    "                        full_prompt = f\"{prompt_text}\\nSummary A:\\n{summary_A}\\nSummary B:\\n{summary_B}\"\n",
    "                        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "\n",
    "                        entry = {\n",
    "                            \"custom_id\" : f\"request-{doctor_index}-{patient_index}-{prompt_index}\",\n",
    "                            \"method\": \"POST\",\n",
    "                            \"url\": \"/v1/chat/completions\",\n",
    "                            \"body\" : {\n",
    "                                \"model\": model,\n",
    "                                \"messages\": messages,\n",
    "                                \"response_format\": { \n",
    "                                    \"type\": \"json_schema\",\n",
    "                                    \"json_schema\": {\n",
    "                                        \"name\": \"validation_response\",\n",
    "                                        \"schema\": {\n",
    "                                            \"type\": \"object\",\n",
    "                                            \"properties\": validation_response\n",
    "                                        }\n",
    "                                    }\n",
    "                                },\n",
    "                                \"reasoning_effort\": \"high\",\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        batch_file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(f\"[Skipped] {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Error] D{doctor_index} P{patient_index} Prompt{prompt_index}: {e}\")\n",
    "    \n",
    "    print(f\"\\n Batch input JSONL written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = ReadFile(\"LLM_as_judge_prompt_5.txt\")\n",
    "\n",
    "output_path = \"batch_o3-mini-high.jsonl\"\n",
    "make_batch_input_jsonl(input_prompt, output_path, \"o3-mini\")\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(output_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send batch file to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "print(batch_input_file_id)\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"test\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(batch.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use, if you need to cancel the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.cancel(\"batch_6833d261ddf48190ae2de54a18fa161d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = client.batches.retrieve('batch_681ab10149dc8190822e78baa1709017')\n",
    "print(batch)\n",
    "print(batch.metadata)\n",
    "print(batch.output_file_id)\n",
    "print(batch.request_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If completed, save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = client.files.content(f\"{batch.output_file_id}\")\n",
    "\n",
    "model_name = \"o3-mini-high\"\n",
    "with open(f\"batch_output_10_runs_reversed_{model_name}_44.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(file_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the results from the jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import csv\n",
    "\n",
    "def calculate_metrics(tp: int, fp: int, fn: int) -> Dict[str, float]:\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1_score}\n",
    "\n",
    "def average_metrics(metrics1: Dict[str, float], metrics2: Dict[str, float]) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"Precision\": (metrics1[\"Precision\"] + metrics2[\"Precision\"]) / 2,\n",
    "        \"Recall\": (metrics1[\"Recall\"] + metrics2[\"Recall\"]) / 2,\n",
    "        \"F1-score\": (metrics1[\"F1-score\"] + metrics2[\"F1-score\"]) / 2,\n",
    "    }\n",
    "\n",
    "def GetSummaryCounts(parsed):\n",
    "    response = parsed['response']['body']['choices'][0]['message']['content']\n",
    "    validation_response = json.loads(response)\n",
    "    summary_counts = validation_response['summary_counts']\n",
    "\n",
    "    tp = summary_counts['TP']\n",
    "    fp = summary_counts['FP']\n",
    "    fn = summary_counts['FN']\n",
    "\n",
    "    return (tp, fp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch results with o3-mini-high with 2 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"batch_output_normal_run_0.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    parsed_normal = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"batch_output_reversed_run_0.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    parsed_reversed = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "csv_filename = \"o3-mini-high-validations-with-2-runs.csv\"\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Doctor\", \"Patient\", \"Precision (Prompt 1)\", \"Recall (Prompt 1)\", \"F1-score (Prompt 1)\", \n",
    "                         \"Precision (Prompt 2)\", \"Recall (Prompt 2)\", \"F1-score (Prompt 2)\", \n",
    "                         \"Precision (Prompt 3)\", \"Recall (Prompt 3)\", \"F1-score (Prompt 3)\",\n",
    "                         \"Precision (Prompt 4)\", \"Recall (Prompt 4)\", \"F1-score (Prompt 4)\",\n",
    "                         \"Precision (Prompt 5)\", \"Recall (Prompt 5)\", \"F1-score (Prompt 5)\"])\n",
    "\n",
    "prompt = 1\n",
    "row = list()\n",
    "for i in range(len(parsed_normal)):\n",
    "    run_id = parsed_normal[i]['custom_id']\n",
    "    print(parsed_normal[i]['custom_id'].split('-'))\n",
    "    print(parsed_reversed[i]['custom_id'].split('-'))\n",
    "\n",
    "    doctor_index = run_id.split('-')[1]\n",
    "    patient_index = run_id.split('-')[2]\n",
    "    prompt_index = run_id.split('-')[3]\n",
    "\n",
    "    print(f\"Processing doctor: {doctor_index}, patient: {patient_index}, prompt: {prompt_index}.\")\n",
    "\n",
    "    if prompt == 1:\n",
    "         row.append(doctor_index)\n",
    "         row.append(patient_index)\n",
    "    tp_normal,fp_normal,fn_normal = GetSummaryCounts(parsed_normal[i])\n",
    "    tp_reversed,fp_reversed,fn_reversed = GetSummaryCounts(parsed_reversed[i])\n",
    "\n",
    "    print(tp_normal,fp_normal,fn_normal)\n",
    "    print(tp_reversed,fp_reversed,fn_reversed)\n",
    "    print(prompt)\n",
    "\n",
    "    metrics_normal = calculate_metrics(tp_normal,fp_normal,fn_normal)\n",
    "    metrics_reversed = calculate_metrics(tp_reversed,fn_reversed,fp_reversed)\n",
    "\n",
    "    avg_metrics = average_metrics(metrics_normal, metrics_reversed)\n",
    "\n",
    "    # Format results for CSV\n",
    "    row.append(round(avg_metrics[\"Precision\"], 3))\n",
    "    row.append(round(avg_metrics[\"Recall\"], 3))\n",
    "    row.append(round(avg_metrics[\"F1-score\"], 3))\n",
    "       \n",
    "    if prompt == 5:\n",
    "        with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file, delimiter=\"\\t\")  # Use tab separator\n",
    "            writer.writerow(row)\n",
    "        row = list()\n",
    "        prompt = 1\n",
    "    else:\n",
    "        prompt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o3-mini-high multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"o3-mini-high-validations-2-way-multiple-runs.csv\"\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Doctor\", \"Patient\", \"Precision (Prompt 1)\", \"Recall (Prompt 1)\", \"F1-score (Prompt 1)\", \n",
    "                         \"Precision (Prompt 2)\", \"Recall (Prompt 2)\", \"F1-score (Prompt 2)\", \n",
    "                         \"Precision (Prompt 3)\", \"Recall (Prompt 3)\", \"F1-score (Prompt 3)\",\n",
    "                         \"Precision (Prompt 4)\", \"Recall (Prompt 4)\", \"F1-score (Prompt 4)\",\n",
    "                         \"Precision (Prompt 5)\", \"Recall (Prompt 5)\", \"F1-score (Prompt 5)\"])\n",
    "\n",
    "\n",
    "for file_number in range(1,11):\n",
    "\n",
    "    with open(f\"batch_output_10_runs_normal_o3-mini-high_{file_number}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        parsed_normal = [json.loads(line) for line in f]\n",
    "\n",
    "    with open(f\"batch_output_10_runs_reversed_o3-mini-high_{file_number}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        parsed_reversed = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "    prompt = 1\n",
    "    row = list()\n",
    "    for i in range(len(parsed_normal)):\n",
    "        run_id = parsed_normal[i]['custom_id']\n",
    "\n",
    "\n",
    "        doctor_index = run_id.split('-')[1]\n",
    "        patient_index = run_id.split('-')[2]\n",
    "        prompt_index = run_id.split('-')[3]\n",
    "\n",
    "        print(f\"Processing doctor: {doctor_index}, patient: {patient_index}, prompt: {prompt_index}.\")\n",
    "\n",
    "        if prompt == 1:\n",
    "            row.append(doctor_index)\n",
    "            row.append(patient_index)\n",
    "        tp_normal,fp_normal,fn_normal = GetSummaryCounts(parsed_normal[i])\n",
    "        tp_reversed,fp_reversed,fn_reversed = GetSummaryCounts(parsed_reversed[i])\n",
    "\n",
    "        print(tp_normal,fp_normal,fn_normal)\n",
    "        print(tp_reversed,fp_reversed,fn_reversed)\n",
    "        print(prompt)\n",
    "\n",
    "        metrics_normal = calculate_metrics(tp_normal,fp_normal,fn_normal)\n",
    "        metrics_reversed = calculate_metrics(tp_reversed,fn_reversed,fp_reversed)\n",
    "\n",
    "        avg_metrics = average_metrics(metrics_normal, metrics_reversed)\n",
    "\n",
    "        # Format results for CSV\n",
    "        row.append(round(avg_metrics[\"Precision\"], 3))\n",
    "        row.append(round(avg_metrics[\"Recall\"], 3))\n",
    "        row.append(round(avg_metrics[\"F1-score\"], 3))\n",
    "        \n",
    "        if prompt == 5:\n",
    "            with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file, delimiter=\"\\t\")  # Use tab separator\n",
    "                writer.writerow(row)\n",
    "            row = list()\n",
    "            prompt = 1\n",
    "        else:\n",
    "            prompt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copmaring different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relevant input files have to be created first\n",
    "models = [\"o3-mini-high\", \"o3-mini-medium\", \"o3-mini-low\", \"o1\", \"gpt-4o\"]\n",
    "\n",
    "batch_ids = list()\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(f\"batch_{model}.jsonl\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"comparing model {model}\"\n",
    "        }\n",
    "    )\n",
    "    print(batch.id)\n",
    "    batch_ids.append(batch.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id in batch_ids:\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(batch)\n",
    "    print(batch.metadata)\n",
    "    print(batch.output_file_id)\n",
    "    print(batch.request_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the relevant batch output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = client.files.content(\"file-\") # Get the corresponding output_file_id\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "with open(f\"batch_output_{model_name}_5.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(file_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different models 10 runs results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "csv_filename = f\"model_{model}_10_runs.csv\"\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Doctor\", \"Patient\", \"Precision\", \"Recall\", \"F1-score \"])\n",
    "\n",
    "for i in range(1,11):\n",
    "    with open(f\"batch_output_{model}_{i}.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        parsed = [json.loads(line) for line in f]\n",
    "\n",
    "    row = list()\n",
    "    for i in range(len(parsed)):\n",
    "        run_id = parsed[i]['custom_id']\n",
    "\n",
    "        doctor_index = run_id.split('-')[1]\n",
    "        patient_index = run_id.split('-')[2]\n",
    "        prompt_index = run_id.split('-')[3]\n",
    "\n",
    "        print(f\"Processing doctor: {doctor_index}, patient: {patient_index}, prompt: {prompt_index}.\")\n",
    "\n",
    "\n",
    "        row.append(doctor_index)\n",
    "        row.append(patient_index)\n",
    "        tp,fp,fn = GetSummaryCounts(parsed[i])\n",
    "\n",
    "        print(tp,fp,fn)\n",
    "\n",
    "        metrics = calculate_metrics(tp,fp,fn)\n",
    "\n",
    "        row.append(round(metrics[\"Precision\"], 3))\n",
    "        row.append(round(metrics[\"Recall\"], 3))\n",
    "        row.append(round(metrics[\"F1-score\"], 3))\n",
    "\n",
    "        with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file, delimiter=\"\\t\")  # Use tab separator\n",
    "            writer.writerow(row)\n",
    "        row = list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aire-medisoft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
